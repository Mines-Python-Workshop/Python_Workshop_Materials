{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Workshop Day 2 \n",
    "Welcome to the Chemistry Department Programming Workshop Day 2!  \n",
    "   \n",
    "These tutorials will focus on more advanced data analysis and visualization tools, and give some time for integration with your own data.\n",
    "  \n",
    "Day 2 consists of 4 sub-modules:  \n",
    "* [`2.0 Data Processing and Analysis`](./2.0_day2_data_analysis_and_stats.ipynb) – statistical analysis, regression and smoothing  \n",
    "* [`2.1 Complex Formatting and Data Visualization`](./2.1_day2_complex_formatting.md) – using class strutures to access complex formatting \n",
    "* [`2.2 Prompt Engineering for Generative AI`](./2.2_day2_prompt_engineering_for_generative_ai.md) – how to write effective prompts for GPT and interpret the code it writes\n",
    "* [`2.3 Applications to Your Data`](./2.3_day2_applications_to_your_data.md) – try to apply workshop tools to you own datasets!\n",
    "\n",
    "## 2.0 Data Processing and Analysis\n",
    "---\n",
    "\n",
    "**Contents**  \n",
    "  \n",
    "In this module, you will complete:\n",
    " * [2.0.1 Simple Linear Regression](#211-interpreting-class-structures)\n",
    " * [2.0.2 Add Error Bars to Matplotlib Plots](#212-writing-a-custom-plotting-class)\n",
    " * [2.0.3 Polynomial Curve Fitting](#212-writing-a-custom-plotting-class)\n",
    " * [2.0.4 Gaussian Process Regression](#212-writing-a-custom-plotting-class)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.1 Simple Linear Regression\n",
    "  \n",
    "In this section, we will use the popular data science/machine learning package [**scikit-learn**](https://scikit-learn.org/stable/). If you do not already have it added to your python environment add it now.  \n",
    " \n",
    "**scikit-learn** has many nice built-in functions for regression, dimensionality reduction (see [PCA](https://www.nature.com/articles/s43586-022-00184-w#:~:text=Principal%20component%20analysis%20is%20a,variance%20of%20all%20the%20variables.)), clustering, and image processing, among other methods. Note also that you can perform the same actions using [scipy's implementation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html), or code the actual matrix operations yourself using numpy.   \n",
    "  \n",
    "Let's first create some sample data:\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_data = np.linspace(start=-20, stop=25, num=100)\n",
    "y_data = x_data**3 + 2000*np.sin(x_data)\n",
    "plt.plot(x_data,y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will fit the data using scikit-learn linear regressor. The `.predict` method works for all scikit learn `Regressor()` type objects, and is one way they remain internally consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_linreg = LinearRegression()\n",
    "\n",
    "my_linreg.fit(x_data.reshape(-1,1), \n",
    "              y_data.reshape(-1,1))\n",
    "\n",
    "y_estim = my_linreg.predict(x_data.reshape(-1,1))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_data, y_estim,'r-', label=\"Linear Estimate\")\n",
    "ax.plot(x_data, y_data, 'teal', label=\"Original Function\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the Pearson Correlation coefficient (R squared) using scikit-learn. Note that there are other types of correlation coefficients that may be more appropriate for your data, and beware [*Anscombe's Quartet*](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_data, y_estim)\n",
    "\n",
    "# here we use an 'f-string' to print the data\n",
    "# f-strings allow us to specify formatting\n",
    "# here we use \":.3\" to specify a float with 3 decimal places\n",
    "print(f\"The Pearson Correlation Coefficient is: \\t {r2:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add this information to the plot as legend text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles, labels = ax.get_legend_handles_labels()\n",
    "labels[0] = f\"Linear Estimate $R^2$ = {r2:.3}\"\n",
    "ax.legend(labels = labels, handles = handles)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.2 Add Error Bars to Matplotlib Plots\n",
    "  \n",
    "In the next example, we will use matplotlib's built in tools to add errorbars to each of our sample data points. We will later shade error regions when we do Gaussian Process Regression (2.0.4). We can also revisit this example and fit a line to our noisy data after section 2.0.3.\n",
    "  \n",
    "First we define a function for our example data:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to fit that is the \"True\" value we don't know. \n",
    "def my_func(x, a, b):\n",
    "    \"\"\"\n",
    "    Always make a description for your function. \n",
    "    \n",
    "    args: \n",
    "        a: a constant multiplier\n",
    "        b: The y intercept\n",
    "        \n",
    "    returns: \n",
    "        The function evaluated at x locations. \n",
    "    \"\"\"\n",
    "    return a * x**3 + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then add some noise to our random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random data\n",
    "np.random.seed(0)\n",
    "x = np.linspace(start=0, \n",
    "                stop=10, \n",
    "                num=30)\n",
    "\n",
    "y = 3 * x**3 + 2 # Invent random values for a and b for the \"True\" fxn\n",
    "\n",
    "noise_amt = .2\n",
    "added_noise = abs(np.random.normal(scale=y*noise_amt, size=x.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data with error bars\n",
    "\n",
    "plt.errorbar(x, \n",
    "             y+added_noise, \n",
    "             yerr=added_noise, \n",
    "             fmt='o', \n",
    "             label='Data with error bars', \n",
    "             ecolor='r', \n",
    "             capsize=5)\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.title('Random Data with Error Bars')\n",
    "\n",
    "plt.text(x=0.5, \n",
    "         y=2000, \n",
    "         s=\"Annotation for when you want it.\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.3 Polynomial Curve Fitting\n",
    "  \n",
    "Here we use scipy instead of scikit-learn to fit a polynomial to noisy sine data. If you want to instead fit a custom function (like Michelis-Menten Kinetics or a Langmuir Isotherm for example), you can tweak your guess function. Alternative curve fitting algorithms such as [universal functions originators](https://en.wikipedia.org/wiki/Symbolic_regression) and [genetic algorithms](https://gplearn.readthedocs.io/en/stable/) are also fun to play around with.\n",
    "  \n",
    "First we define the functions for our example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "import pandas as pd\n",
    "\n",
    "def Some_Unknown_Function1(x, a, b=5):\n",
    "    \"\"\"\n",
    "    Modified sine function. \n",
    "    \n",
    "    args: \n",
    "        x (float); Independent Variable Vals\n",
    "        a (float); Controls amplitude of sine()\n",
    "        b (float); Controls y-offset of function. \n",
    "    \"\"\"\n",
    "    func_value = b + a*np.sin(x)\n",
    "    return func_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Some_Unknown_Function2(x, noiseval=0):\n",
    "    \"\"\"\n",
    "    X squared function with added noise. \n",
    "    \n",
    "    args: \n",
    "        x (float); Independent Variable Vals\n",
    "        noiseval (zero or positive float); determines stdev of Gaussian noise. \n",
    "    \"\"\"\n",
    "    y = x**2\n",
    "    noise = np.random.normal(loc=0, \n",
    "                             scale=noiseval, \n",
    "                             size=len(x))\n",
    "    return y + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create the y values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = np.linspace(start=0, \n",
    "                     stop=10, \n",
    "                     num=55)\n",
    "\n",
    "y_vals = Some_Unknown_Function1(x_vals, \n",
    "                                a=3, \n",
    "                                b=15)\n",
    "\n",
    "y_vals2 = Some_Unknown_Function2(x_vals, \n",
    "                                 noiseval=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_vals, \n",
    "         y_vals2, 'b.', \n",
    "         label=\"Noisy Data\")\n",
    "\n",
    "plt.plot(x_vals, \n",
    "         Some_Unknown_Function2(x_vals, \n",
    "                                noiseval=0), \n",
    "         'k-', \n",
    "         label=\"No Noise Added\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `curve_fit()` method from scipy is used here to get teh function parameters. Note that on real data, you will already have your x and y values. You will then need to identify a functional form written as a python function that can be passed to the `curve_fit()` method. Note also that there may not be a unique solution, and different optimization algorithms may yield different parameters. Further, the same optimization algorithm may yield different results when used on the same data following a log transformation. Be careful when fitting curves, and read the [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_estim = curve_fit(Some_Unknown_Function2, \n",
    "                    xdata=x_vals, \n",
    "                    ydata=y_vals2)\n",
    "# Returns function parameters. Saved as a tuple of popt and pcov - see the documentation to interpret these values\n",
    "y_estim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our parameters, we need to check that our fit is reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_vals, \n",
    "         x_vals**2, \n",
    "         'b.', \n",
    "         label=\"True Function\")\n",
    "\n",
    "plt.plot(x_vals, \n",
    "         y_vals2, \n",
    "         'g*',\n",
    "         label=\"Noisy Measurements\")\n",
    "\n",
    "plt.plot(x_vals, # x points\n",
    "         Some_Unknown_Function2(x_vals, *y_estim[1]), \n",
    "         'r-', # Color and line settings\n",
    "         label=\"Reconstructed Function\"\n",
    "        )\n",
    "\n",
    "plt.text(x=5, # X location of text\n",
    "         y=-10, # Y location of text\n",
    "         s=f\"y popt estimate: {np.round(y_estim[0],4)}\\ncov(popt) estimate: {np.round(y_estim[1],4)}\"\n",
    "        )\n",
    "# popt; i.e.: how many optimal parameters being searched. \n",
    "# cov; i.e.: the covariance of popt. See documentation. \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On your own, see if you can fit a curve to the noisy data from section 2.0.2 above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.4 Gaussian Process Regression\n",
    "  \n",
    "Gaussian Process Regression is another technique applied to curve-fitting. This method is on the 'machine learning' end of the spectrum between datascience/statistics and AI. It can be useful for low dimensional datasets, and also neatly demonstrates error region shading. For more information in Gaussian Processes and the important assumptions and use cases, see this [link](https://scikit-learn.org/stable/modules/gaussian_process.html).  \n",
    "   \n",
    "For now, we'll skip some of the background and get straight to the demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman2\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "X = np.linspace(start=0, stop=10, num=1000).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "\n",
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"True process data\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our sample data generated, we will proceed to making a Gaussian Process `Regressor()` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "training_indices = rng.choice(np.arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "\n",
    "kernel = 1 * RBF(length_scale=1.0, \n",
    "                 length_scale_bounds=(1e-2, 1e2))\n",
    "\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel, \n",
    "                                            n_restarts_optimizer=9)\n",
    "\n",
    "gaussian_process.fit(X_train, \n",
    "                     y_train)\n",
    "\n",
    "gaussian_process.kernel_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the simple linear model, we will use the `predict()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will display the data using shaded error regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=1, \n",
    "                       figsize=(10,10))\n",
    "\n",
    "# Plot original data. \n",
    "ax.plot(X, \n",
    "        y, \n",
    "        label=r\"$f(x) = x \\sin(x)$\", \n",
    "        linestyle=\"dotted\")\n",
    "\n",
    "# Plot noisy data that was subsampled for training set (if using machine learning)\n",
    "ax.scatter(X_train, \n",
    "           y_train, \n",
    "           label=\"Observations\")\n",
    "\n",
    "# Plot mean line from model or curvefit or whatnot. \n",
    "ax.plot(X, \n",
    "        mean_prediction, \n",
    "        label=\"Mean prediction\")\n",
    "\n",
    "# MAKE SOLID ERROR REGION\n",
    "ax.fill_between(\n",
    "    X.ravel(), # Flatten array so it plots. \n",
    "    mean_prediction - 1.96 * std_prediction, # Define lower 95% CI limit\n",
    "    mean_prediction + 1.96 * std_prediction, # Define upper 95% CI limit\n",
    "    alpha=0.3, # how opaque do you want the error bars (%). \n",
    "    color=\"gray\",\n",
    "    label=r\"95% confidence interval\", # Label the shaded region for inclusion in legend\n",
    ")\n",
    "\n",
    "# Plot decorations (i.e.: legend, labels, title, etc.)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$f(x)$\")\n",
    "_ = ax.set_title(\"Gaussian process regression on noise-free dataset\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note again!  \n",
    "There is a difference between the confidence interval and the prediction interval. Be sure to [read up on this](https://towardsdatascience.com/how-confidence-and-prediction-intervals-work-4592019576d8) before using either one. When calculating either interval, you make fundamental assumptions about the distribution of your data- ensure that these assumptions are appropriate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Congratulations!  \n",
    "You have now completed [**2.0 Data Processing And Analysis**]().  \n",
    "Please proceed to [**2.1 Complex Formatting and Data Visualization**](./2.1_day2_complex_formatting.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psi4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
